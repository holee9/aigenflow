"""
Real AI Response Reproducibility Evaluation.
Uses Playwright gateway to test actual AI responses.

Note: This requires valid Playwright sessions with all AI providers.
"""

import asyncio
import hashlib
import statistics
import sys
from datetime import datetime
from pathlib import Path
from typing import Any

sys.path.insert(0, '.')

from src.templates.manager import TemplateManager


class RealAIReproducibilityEvaluator:
    """Evaluate actual AI response reproducibility using Playwright gateway."""

    def __init__(self):
        self.iterations = 10  # Reduced from 10 to 3 for faster testing
        self.results_dir = Path("docs/ai-reproducibility-results/actual")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.template_manager = TemplateManager()

    def get_test_prompt(self) -> str:
        """Generate a test prompt from template."""
        context = {
            "topic": "AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸í° ê´€ë¦¬ ì‹œìŠ¤í…œ",
            "doc_type": "bizplan",
            "language": "ko",
        }
        # Use brainstorm template as test prompt
        return self.template_manager.render_prompt("phase_1/brainstorm_chatgpt", context)

    def calculate_similarity(self, text1: str, text2: str) -> dict[str, float]:
        """Calculate similarity between two AI responses."""
        # 1. Length similarity
        len1, len2 = len(text1), len(text2)
        len_sim = min(len1, len2) / max(len1, len2) if max(len1, len2) > 0 else 1.0

        # 2. Word overlap (Jaccard)
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        if words1 or words2:
            jaccard = len(words1 & words2) / len(words1 | words2)
        else:
            jaccard = 1.0

        # 3. Character-level similarity (first 500 chars)
        sample_size = 500
        sample1, sample2 = text1[:sample_size], text2[:sample_size]
        char_match = sum(c1 == c2 for c1, c2 in zip(sample1, sample2)) / sample_size

        # 4. Hash match
        hash1 = hashlib.md5(text1.encode()).hexdigest()
        hash2 = hashlib.md5(text2.encode()).hexdigest()
        hash_match = 1.0 if hash1 == hash2 else 0.0

        return {
            "length_similarity": len_sim,
            "jaccard_similarity": jaccard,
            "character_match": char_match,
            "hash_match": hash_match,
            "overall_score": (len_sim * 0.2 + jaccard * 0.4 + char_match * 0.3 + hash_match * 0.1),
        }

    async def evaluate_single_ai(self, provider_name: str, prompt: str) -> dict[str, Any]:
        """Evaluate single AI provider reproducibility."""
        print(f"\n{'='*60}")
        print(f"Evaluating {provider_name}")
        print(f"Prompt length: {len(prompt)} chars")
        print(f"Iterations: {self.iterations}")
        print(f"{'='*60}")

        results = {
            "provider_name": provider_name,
            "prompt": prompt[:200] + "...",
            "iterations": self.iterations,
            "responses": [],
            "response_times": [],
            "errors": [],
        }

        # This would require actual Playwright provider implementation
        # Framework structure provided:
        # provider = self.get_provider(provider_name)
        # if not provider:
        #     results["status"] = "provider_not_available"
        #     results["error"] = f"{provider_name} provider not configured"
        #     return results

        # for i in range(self.iterations):
        #     try:
        #         start_time = datetime.now()
        #         response = await provider.send_message(prompt)
        #         end_time = datetime.now()
        #
        #         results["responses"].append(response)
        #         results["response_times"].append((end_time - start_time).total_seconds())
        #
        #         print(f"  Iteration {i+1}/{self.iterations}: {len(response)} chars, {(end_time - start_time).total_seconds():.2f}s")
        #
        #     except Exception as e:
        #         results["errors"].append(str(e))
        #         print(f"  Iteration {i+1}/{self.iterations}: ERROR - {e}")

        # Calculate similarities if we have responses
        if len(results["responses"]) >= 2:
            similarities = []
            for i in range(len(results["responses"]) - 1):
                sim = self.calculate_similarity(results["responses"][i], results["responses"][i+1])
                similarities.append(sim)

            results["similarities"] = similarities
            results["avg_similarity"] = {
                "length": statistics.mean([s["length_similarity"] for s in similarities]),
                "jaccard": statistics.mean([s["jaccard_similarity"] for s in similarities]),
                "character": statistics.mean([s["character_match"] for s in similarities]),
                "hash": statistics.mean([s["hash_match"] for s in similarities]),
                "overall": statistics.mean([s["overall_score"] for s in similarities]),
            }

        results["status"] = "evaluated" if len(results["responses"]) > 0 else "no_responses"
        return results

    async def evaluate_all_providers(self, prompt: str) -> list[dict[str, Any]]:
        """Evaluate all AI providers."""
        providers = ["chatgpt", "claude", "gemini", "perplexity"]
        results = []

        for provider in providers:
            try:
                result = await self.evaluate_single_ai(provider, prompt)
                results.append(result)
            except Exception as e:
                results.append({
                    "provider_name": provider,
                    "status": "error",
                    "error": str(e),
                })

        return results

    def generate_report(self, results: list[dict[str, Any]]) -> str:
        """Generate evaluation report."""
        lines = []
        lines.append("# ì‹¤ì œ AI ì‘ë‹µ ì¬í˜„ì„± í‰ê°€ ë³´ê³ ì„œ")
        lines.append("")
        lines.append(f"**í‰ê°€ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append("**í‰ê°€ ë°©ì‹**: Playwright ì›¹ ë¸Œë¼ìš°ì € ê²Œì´íŠ¸ì›¨ì´")
        lines.append(f"**ë°˜ë³µ íšŸìˆ˜**: {self.iterations}íšŒ")
        lines.append("")

        # Summary
        lines.append("## 1. í‰ê°€ ê°œìš”")
        lines.append("")
        lines.append("| AI | ìƒíƒœ | ì‘ë‹µ ìˆ˜ | í‰ê·  ì‘ë‹µ ì‹œê°„ | í‰ê·  ìœ ì‚¬ë„ |")
        lines.append("|----|------|----------|----------------|-------------|")

        for r in results:
            if r["status"] == "evaluated" and "avg_similarity" in r:
                status = "âœ… ì™„ë£Œ"
                responses = len(r["responses"])
                avg_time = f"{statistics.mean(r['response_times']):.1f}s" if r.get("response_times") else "N/A"
                avg_sim = f"{r['avg_similarity']['overall']*100:.1f}%"
            elif r["status"] == "provider_not_available":
                status = "âŒ ë¯¸ì„¤ì •"
                responses = 0
                avg_time = "N/A"
                avg_sim = "N/A"
            else:
                status = f"âš ï¸ {r['status']}"
                responses = len(r.get("responses", []))
                avg_time = "N/A"
                avg_sim = "N/A"

            lines.append(f"| {r['provider_name']} | {status} | {responses} | {avg_time} | {avg_sim} |")

        lines.append("")

        # Detailed results
        lines.append("## 2. ìƒì„¸ ê²°ê³¼")
        lines.append("")

        for r in results:
            lines.append(f"### {r['provider_name'].upper()}")
            lines.append("")
            lines.append(f"- **ìƒíƒœ**: {r['status']}")

            if r.get("errors"):
                lines.append(f"- **ì˜¤ë¥˜**: {', '.join(r['errors'])}")

            if "avg_similarity" in r:
                avg = r["avg_similarity"]
                lines.append("")
                lines.append("**ìœ ì‚¬ë„ ì§€í‘œ**:")
                lines.append(f"- ê¸¸ì´ ìœ ì‚¬ë„: {avg['length']*100:.1f}%")
                lines.append(f"- ë‹¨ì–´ ì¤‘ì²©ë„ (Jaccard): {avg['jaccard']*100:.1f}%")
                lines.append(f"- ë¬¸ì ì¼ì¹˜: {avg['character']*100:.1f}%")
                lines.append(f"- í•´ì‹œ ì¼ì¹˜: {avg['hash']*100:.1f}%")
                lines.append(f"- **ì¢…í•© ìœ ì‚¬ë„: {avg['overall']*100:.1f}%**")

            lines.append("")

        # Conclusion
        lines.append("## 3. ê²°ë¡ ")
        lines.append("")

        evaluated_count = sum(1 for r in results if r["status"] == "evaluated")
        total_count = len(results)

        if evaluated_count == 0:
            lines.append("âš ï¸ **í‰ê°€ ë¯¸ì§„í–‰**: ì‹¤ì œ AI ì‘ë‹µ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ”:")
            lines.append("")
            lines.append("1. Playwright í”„ë¡œí•„ ì„¤ì • (`AigenFlow setup` ì‹¤í–‰)")
            lines.append("2. ê° AI ì„œë¹„ìŠ¤ ì›¹ ë¡œê·¸ì¸")
            lines.append("3. ì„¸ì…˜ ìœ íš¨ì„± í™•ì¸")
        elif evaluated_count == total_count:
            avg_overall = statistics.mean([
                r["avg_similarity"]["overall"] for r in results
                if r.get("avg_similarity")
            ])
            lines.append(f"- **í‰ê°€ ì™„ë£Œ**: {evaluated_count}/{total_count}ê°œ AI")
            lines.append(f"- **í‰ê·  ì¬í˜„ì„±**: {avg_overall*100:.1f}%")

            if avg_overall >= 0.9:
                grade = "A+"
                assessment = "ë§¤ìš° ìš°ìˆ˜í•œ ì¬í˜„ì„±"
            elif avg_overall >= 0.8:
                grade = "A"
                assessment = "ìš°ìˆ˜í•œ ì¬í˜„ì„±"
            elif avg_overall >= 0.7:
                grade = "B"
                assessment = "ì–‘í˜¸í•œ ì¬í˜„ì„±"
            else:
                grade = "C"
                assessment = "ê°œì„  í•„ìš”"

            lines.append(f"- **í’ˆì§ˆ ë“±ê¸‰**: {grade} ({assessment})")
        else:
            lines.append(f"- **ë¶€ë¶„ í‰ê°€**: {evaluated_count}/{total_count}ê°œ AIë§Œ ì™„ë£Œ")

        lines.append("")
        lines.append("*ë³´ê³ ì„œ ìƒì„±ì¼: 2026-02-15*")

        return "\n".join(lines)


async def main():
    """Main evaluation function."""
    print("ğŸ” ì‹¤ì œ AI ì‘ë‹µ ì¬í˜„ì„± í‰ê°€")
    print("=" * 60)
    print()
    print("Playwright ê²Œì´íŠ¸ì›¨ì´ë¥¼ í†µí•œ ì‹¤ì œ AI ì‘ë‹µ í‰ê°€")
    print()
    print("âš ï¸ ì£¼ì˜ì‚¬í•­:")
    print("  - ê° AI ì„œë¹„ìŠ¤ì— ë¡œê·¸ì¸ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤")
    print("  - í‰ê°€ì—ëŠ” ìƒë‹¹í•œ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤")
    print("  - API ìš”ì²­ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print()
    print("=" * 60)

    evaluator = RealAIReproducibilityEvaluator()
    prompt = evaluator.get_test_prompt()

    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
    print("   (í…œí”Œë¦¿: phase_1/brainstorm_chatgpt)")
    print()

    # Check if we should run actual evaluation
    # For now, generate framework report without actual API calls
    # results = await evaluator.evaluate_all_providers(prompt)

    # Generate placeholder report
    results = [
        {
            "provider_name": "chatgpt",
            "status": "framework_only",
            "note": "ì‹¤ì œ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” Playwright ì„¸ì…˜ í•„ìš”"
        },
        {
            "provider_name": "claude",
            "status": "framework_only",
            "note": "ì‹¤ì œ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” Playwright ì„¸ì…˜ í•„ìš”"
        },
        {
            "provider_name": "gemini",
            "status": "framework_only",
            "note": "ì‹¤ì œ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” Playwright ì„¸ì…˜ í•„ìš”"
        },
        {
            "provider_name": "perplexity",
            "status": "framework_only",
            "note": "ì‹¤ì œ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” Playwright ì„¸ì…˜ í•„ìš”"
        },
    ]

    report = evaluator.generate_report(results)

    # Save report
    report_path = evaluator.results_dir / "ai-response-reproducibility-report.md"
    report_path.parent.mkdir(parents=True, exist_ok=True)
    report_path.write_text(report, encoding="utf-8")

    print(f"\nğŸ“„ í‰ê°€ ë³´ê³ ì„œ ì €ì¥ë¨: {report_path}")
    print()
    print("ì‹¤ì œ í‰ê°€ ì‹¤í–‰ ë°©ë²•:")
    print("  1. AigenFlow setup - ê° AI ì„œë¹„ìŠ¤ ë¡œê·¸ì¸")
    print("  2. AigenFlow check - ì„¸ì…˜ ìƒíƒœ í™•ì¸")
    print("  3. python tests/test_real_ai_reproducibility.py")


if __name__ == "__main__":
    asyncio.run(main())
